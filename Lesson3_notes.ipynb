{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6e2c97",
   "metadata": {},
   "source": [
    "# Lesson 3 list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce686b1",
   "metadata": {},
   "source": [
    "列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733a771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
      "[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]\n",
      "[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]\n",
      "[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]\n"
     ]
    }
   ],
   "source": [
    "# 生成等差数列\n",
    "n = list(range(1,20,2))\n",
    "print(n)\n",
    "\n",
    "# 每个元素乘方\n",
    "# 列表不能直接进行数学运算\n",
    "# 法一:循环加入元素\n",
    "m = []\n",
    "for i in range(10):\n",
    "    m.append((2 * i + 1) ** 2)\n",
    "print(m)\n",
    "\n",
    "# 法二：使用上面的列表 + 循环\n",
    "k = []\n",
    "for i in n: # 循环可以取列表中每一个值\n",
    "    k.append(i ** 2)\n",
    "print(k)\n",
    "\n",
    "# 法三：巧妙的列表构建法\n",
    "a = [x ** 2 for x in n]\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ea2af",
   "metadata": {},
   "source": [
    "字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752ea96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 10]\n",
      "{3: 'a', 5: 'c', 10: 'b'}\n"
     ]
    }
   ],
   "source": [
    "d = {10:'b',3:'a',5:'c'}\n",
    "nd = {k:d[k] for k in sorted(d)}\n",
    "print(sorted(d))\n",
    "print(nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52acf9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 5), ('b', 10), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "d = {10:'b',3:'a',5:'c'}\n",
    "nd = sorted([(v,k) for (k,v) in d.items()],reverse = True)\n",
    "print(nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdd327",
   "metadata": {},
   "source": [
    "## 随堂练习\n",
    "\n",
    "统计下列语句中各字母的出现次数:\n",
    "\n",
    "sentence =“A man that is young in years, may be old \n",
    "in hours, if he has lost no time.“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 6, 'b': 1, 'c': 0, 'd': 1, 'e': 4, 'f': 1, 'g': 1, 'h': 4, 'i': 5, 'j': 0, 'k': 0, 'l': 2, 'm': 3, 'n': 5, 'o': 5, 'p': 0, 'q': 0, 'r': 2, 's': 5, 't': 4, 'u': 2, 'v': 0, 'w': 0, 'x': 0, 'y': 3, 'z': 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence = 'A man that is young in years, may be old in hours, if he has lost no time.'\n",
    "\n",
    "sentence_new = sentence.lower()\n",
    "keys = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "values = np.zeros(26)\n",
    "values = list(values)\n",
    "alp_dict = dict(zip(keys,values))\n",
    "\n",
    "for key in alp_dict.keys(): # 注意这个最后的括号\n",
    "    alp_dict[key] = sentence_new.count(key)\n",
    "\n",
    "print(alp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ce9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "y\n",
      "e\n",
      "a\n",
      "r\n",
      "s\n",
      ",\n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "o\n",
      "l\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "h\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      ",\n",
      "i\n",
      "f\n",
      " \n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "l\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      ".\n",
      "{'a': 5, 'e': 4, 'g': 1, 'h': 4, 'i': 5, 'm': 3, 'n': 5, 'o': 5, 's': 5, 't': 4, 'u': 2, 'y': 3}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'A man that is young in years,may be old in hours,if he has lost no time.'\n",
    "# 法一：\n",
    "# a = sentence.count('a') + sentence.count('A')\n",
    "\n",
    "# 法二：对字符串中的元素进行遍历\n",
    "for i in sentence:\n",
    "     print(i)\n",
    "\n",
    "book = dict(a=0,e=0,g=0,h=0,i=0,m=0,n=0,o=0,s=0,t=0,u=0,y=0)  # 这样创建的话key的部分不需要打引号\n",
    "for key in book.keys():\n",
    "     book[key] = sentence.count(key)\n",
    "print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n', 'g', 'r', 'u', 'f', 'l', ',', 'o', 'A', 'h', 't', 'y', 'b', 'e', '.', 's', 'a', ' ', 'm', 'i', 'd'}\n",
      "5 1 2 2 1 2\n"
     ]
    }
   ],
   "source": [
    "sentence = 'A man that is young in years,may be old in hours,if he has lost no time.'\n",
    "x = set(sentence)\n",
    "print(x)\n",
    "\n",
    "n = sentence.count('n')            # 或循环\n",
    "g = sentence.count('g')\n",
    "r = sentence.count('r')\n",
    "u = sentence.count('u')\n",
    "f = sentence.count('f')\n",
    "l = sentence.count('l')\n",
    "# ...............\n",
    "print(n,g,r,u,f,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecf4b1",
   "metadata": {},
   "source": [
    "## 词频提取\n",
    "\n",
    "将会议讲话英文稿(http://172.16.85.56/wiki)保存为文本，找出其中出现频率最高的10个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6001f2e",
   "metadata": {},
   "source": [
    "### > 读取文本文件并处理注意事项\n",
    "\n",
    "- 注意换行符`'\\n'`的存在（替换）\n",
    "\n",
    "对于该题，注意：\n",
    "\n",
    "- 特殊标点的转换，如`,.;“”`\n",
    "- 转换为小写（有些单词是一个单词，只不过有大写有小写）\n",
    "\n",
    "### > 排序\n",
    "\n",
    "- 基础方法：冒泡排序法、选择排序法\n",
    "- 特殊函数：\n",
    "\n",
    "> 常见数据结构的排序方法总结\n",
    "\n",
    "| 适用场合 | 函数 | 用法说明 | 注意事项 |\n",
    "|----------|------|----------|----------|\n",
    "| 列表  | `a.sort()` / `sorted(a)` | 将列表中的元素按从小到大排序 | 列表中元素类型必须可比较 |\n",
    "| 字典  | `nd = {k: d[k] for k in sorted(d)}`<br>`nd = sorted([(v, k) for (k, v) in d.items()], reverse=True)` | 按键排序 / 按值排序 ||\n",
    "| 数组  | `x.argsort()` / `np.argsort(x)` | 返回数组按值排序后的索引 | 对于第一个，x得是numpy数组 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842491cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 'the'), (54, 'and'), (42, 'of'), (36, 'to'), (21, 'people'), (20, 'our'), (19, 'party'), (15, 'is'), (14, 'in'), (13, 'with'), (13, 'a'), (11, 'great'), (10, 'we'), (10, 'that'), (10, 'nation'), (10, 'be'), (9, 'responsibility'), (9, 'on'), (9, 'more'), (8, 'work'), (8, 'have'), (8, 'chinese'), (8, 'all'), (7, 'you'), (7, 'world'), (7, 'has'), (7, 'for'), (7, 'comrade'), (7, 'committee'), (7, 'as'), (6, 'members'), (6, 'cpc'), (5, 'new'), (5, 'china'), (5, 'central'), (5, 'by'), (5, 'but'), (5, 'better'), (5, 'are'), (4, 'will'), (4, 'up'), (4, 'this'), (4, 'standing'), (4, 'press'), (4, 'political'), (4, 'must'), (4, 'meeting'), (4, 'i'), (4, 'history'), (4, 'groups'), (4, 'friends'), (4, 'ethnic'), (4, 'during'), (4, 'around'), (3, 'us'), (3, 'trust'), (3, 'there'), (3, 'revival'), (3, 'one'), (3, 'masses'), (3, 'make'), (3, 'long'), (3, 'live'), (3, 'life'), (3, 'leadership'), (3, 'hard'), (3, 'good'), (3, 'every'), (3, 'country'), (3, 'congress'), (3, 'comrades'), (3, 'bureau'), (3, 'beautiful'), (2, 'zhang'), (2, 'within'), (2, 'wholeheartedly'), (2, 'whole'), (2, 'while'), (2, 'well'), (2, 'very'), (2, 'unite'), (2, 'time'), (2, 'they'), (2, 'their'), (2, 'thank'), (2, 'than'), (2, 'strive'), (2, 'social'), (2, 'since'), (2, 'served'), (2, 'secretary'), (2, 'proud'), (2, 'process'), (2, 'problems'), (2, 'plenary'), (2, 'own'), (2, 'other'), (2, 'organisation'), (2, 'not'), (2, 'no'), (2, 'never'), (2, 'needs'), (2, 'me'), (2, 'many'), (2, 'mankind'), (2, 'made'), (2, 'limited'), (2, 'li'), (2, 'let'), (2, 'led'), (2, 'learn'), (2, 'lead'), (2, 'keqiang'), (2, 'into'), (2, 'hope'), (2, 'here'), (2, 'general'), (2, 'from'), (2, 'elected'), (2, 'effort'), (2, 'diligence'), (2, 'difficulties'), (2, 'day'), (2, 'contribution'), (2, 'continue'), (2, 'civilisation'), (2, 'behalf'), (2, 'at'), (2, 'an'), (2, 'always'), (2, 'also'), (2, 'about'), (1, 'zhengsheng'), (1, 'yunshan'), (1, 'yu'), (1, 'yesterday'), (1, 'years'), (1, 'yearning'), (1, 'xi'), (1, 'would'), (1, 'woe'), (1, 'wisdom'), (1, 'who'), (1, 'where'), (1, 'weightier'), (1, 'weal'), (1, 'way'), (1, 'wang'), (1, 'waiting'), (1, 'voice'), (1, 'vigilant'), (1, 'victoriously'), (1, 'unprecedented'), (1, 'unleash'), (1, 'united'), (1, 'undue'), (1, 'understanding'), (1, 'understand'), (1, 'turned'), (1, 'tremendous'), (1, 'transforming'), (1, 'towards'), (1, 'touch'), (1, 'ties'), (1, 'thinking'), (1, 'these'), (1, 'then'), (1, 'them'), (1, 'text'), (1, 'tenaciously'), (1, 'take'), (1, 'tai'), (1, 'supervises'), (1, 'such'), (1, 'style'), (1, 'struggle'), (1, 'strong'), (1, 'strict'), (1, 'strength'), (1, 'steadfastly'), (1, 'standard'), (1, 'stand'), (1, 'stable'), (1, 'speech'), (1, 'sorry'), (1, 'some'), (1, 'socialism'), (1, 'so'), (1, 'six'), (1, 'situation'), (1, 'sincerely'), (1, 'sincere'), (1, 'shoulders'), (1, 'sheet'), (1, 'share'), (1, 'severe'), (1, 'seven'), (1, 'serving'), (1, 'serves'), (1, 'security'), (1, 'secretariat'), (1, 'scoring'), (1, 'satisfactory'), (1, 'rose'), (1, 'road'), (1, 'rest'), (1, 'resolved'), (1, 'resolve'), (1, 'resolute'), (1, 'resist'), (1, 'relying'), (1, 'reliable'), (1, 'reform'), (1, 'reason'), (1, 'realising'), (1, 'realise'), (1, 'real'), (1, 'reached'), (1, 'rally'), (1, 'qishan'), (1, 'prosperous'), (1, 'prosperity'), (1, 'prospects'), (1, 'prominent'), (1, 'professional'), (1, 'productive'), (1, 'production'), (1, 'produced'), (1, 'pressing'), (1, 'powerfully'), (1, 'politburo'), (1, 'placing'), (1, 'placed'), (1, 'period'), (1, 'overcome'), (1, 'out'), (1, 'order'), (1, 'opening'), (1, 'opened'), (1, 'only'), (1, 'old'), (1, 'now'), (1, 'night'), (1, 'need'), (1, 'nations'), (1, 'national'), (1, 'my'), (1, 'mutual'), (1, 'mount'), (1, 'most'), (1, 'modern'), (1, 'mission'), (1, 'mind'), (1, 'metal'), (1, 'member'), (1, 'meet'), (1, 'medical'), (1, 'major'), (1, 'maintains'), (1, 'love'), (1, 'lots'), (1, 'lofty'), (1, 'living'), (1, 'liu'), (1, 'limit'), (1, 'like'), (1, 'laurels'), (1, 'ladies'), (1, 'labour'), (1, 'known'), (1, 'kept'), (1, 'just'), (1, 'journey'), (1, 'jobs'), (1, 'jinping'), (1, 'itself'), (1, 'its'), (1, 'it'), (1, 'issues'), (1, 'iron'), (1, 'introduce'), (1, 'insist'), (1, 'individuals'), (1, 'individual'), (1, 'indelible'), (1, 'income'), (1, 'improves'), (1, 'impoverished'), (1, 'ideals'), (1, 'however'), (1, 'home'), (1, 'higher'), (1, 'heroes'), (1, 'heavy'), (1, 'heart'), (1, 'harmony'), (1, 'hardworking'), (1, 'hardship'), (1, 'happy'), (1, 'happiness'), (1, 'hall'), (1, 'grow'), (1, 'greater'), (1, 'gratitude'), (1, 'gradually'), (1, 'goal'), (1, 'gentlemen'), (1, 'gaoli'), (1, 'further'), (1, 'full'), (1, 'founding'), (1, 'fostered'), (1, 'formality'), (1, 'forces'), (1, 'first'), (1, 'firmly'), (1, 'firm'), (1, 'fight'), (1, 'failed'), (1, 'fades'), (1, 'faces'), (1, 'faced'), (1, 'express'), (1, 'experienced'), (1, 'expectations'), (1, 'expect'), (1, 'excellent'), (1, 'everyone'), (1, 'especially'), (1, 'era'), (1, 'environment'), (1, 'entire'), (1, 'ensuring'), (1, 'enforces'), (1, 'encouragement'), (1, 'emphasis'), (1, 'emancipate'), (1, 'election'), (1, 'efforts'), (1, 'effectively'), (1, 'education'), (1, 'earnestly'), (1, 'duties'), (1, 'doing'), (1, 'discipline'), (1, 'diligent'), (1, 'difficulty'), (1, 'development'), (1, 'develop'), (1, 'demonstrated'), (1, 'deliver'), (1, 'dejiang'), (1, 'deeply'), (1, 'deepening'), (1, 'dedicated'), (1, 'deals'), (1, 'days'), (1, 'dangerous'), (1, 'culture'), (1, 'created'), (1, 'create'), (1, 'coverage'), (1, 'courage'), (1, 'countries'), (1, 'countless'), (1, 'corruption'), (1, 'core'), (1, 'conveyed'), (1, 'contributions'), (1, 'continuing'), (1, 'constant'), (1, 'conducted'), (1, 'conduct'), (1, 'conditions'), (1, 'concluded'), (1, 'concerted'), (1, 'complacent'), (1, 'communist'), (1, 'common'), (1, 'comfortable'), (1, 'comes'), (1, 'colleagues'), (1, 'close'), (1, 'children'), (1, 'characteristics'), (1, 'challenges'), (1, 'cause'), (1, 'care'), (1, 'capture'), (1, 'capability'), (1, 'cannot'), (1, 'can'), (1, 'cadres'), (1, 'bureaucracy'), (1, 'burden'), (1, 'bright'), (1, 'both'), (1, 'bit'), (1, 'between'), (1, 'being'), (1, 'beijing'), (1, 'been'), (1, 'become'), (1, 'baton'), (1, 'backward'), (1, 'attention'), (1, 'attend'), (1, 'assigned'), (1, 'arduous'), (1, 'any'), (1, 'answer'), (1, 'another'), (1, 'among'), (1, 'am'), (1, 'ahead'), (1, 'after'), (1, 'advancing'), (1, 'advancement'), (1, 'advance'), (1, 'addressed'), (1, 'accomplishments'), (1, 'accepting'), (1, 'abundance')]\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:\\\\my study\\\\Python Study\\\\Lesson\\\\Practice1.txt'\n",
    "file = open(filename,'r')\n",
    "data = file.read()\n",
    "# 特殊字符，可以再加上;  以及可以全部转为小写\n",
    "voc = data.replace('\\n',' ').replace(',','').replace('.','').replace('“','').replace('”','').replace(';','').lower().split(' ')  \n",
    "dict = {}\n",
    "for i in voc:\n",
    "    if i.isalpha(): # 有字母的才会被放在字典里\n",
    "        dict[i] = voc.count(i)\n",
    "\n",
    "file.close()\n",
    "\n",
    "## 或者这样\n",
    "# m = {}\n",
    "# for i in data:\n",
    "#     if i in m: # 默认对键作判断\n",
    "#         m[i] = m[i] +1\n",
    "#     else:\n",
    "#         m[i] = 1\n",
    "\n",
    "# 字典排序\n",
    "nd = sorted([(v,k) for (k,v) in dict.items()],reverse= True)# 按照值排序，并且逆序\n",
    "print(nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0e5fa",
   "metadata": {},
   "source": [
    "## 文本读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7979cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['打开终端：windows+R  →  cmd  →  输入python可以进入python环境', '', '终端下的DOS命令：', 'D:     改变路径到D盘', 'cd  改变目录', 'cd ..返回上层目录', 'dir    列出当前目录下文件和文件夹', 'del 删除文件', 'python 进入python运行环境', 'ipython 进入ipython增强运行环境', 'python ....py 运行脚本', 'exit() 退出终端或python环境', '', '注意缩进']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"D:\\\\my study\\\\Python Study\\\\Lesson\\\\终端使用.txt\",'r')\n",
    "text = f.read().split(\"\\n\")\n",
    "print(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a248fc",
   "metadata": {},
   "source": [
    "### 将一百以内的奇数保存为文本文件，从这个文本文件中读取数据，计算这些数的平方，保存为第二列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2361cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'D:\\\\my study\\\\Python Study\\\\Lesson\\\\Practice2.txt'\n",
    "\n",
    "file = open(filename,'w')\n",
    "for i in range(1,100,2):\n",
    "    file.write(str(i) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc275b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename,'r')\n",
    "data = file.read()\n",
    "data_new = data.replace('\\n',' ').strip().split()\n",
    "file.close()\n",
    "file = open(filename,'w')  # 读入并且写入记得一定一定不要漏了后面重新设置是写进的权限\n",
    "for i in data_new:\n",
    "    j = float(i) ** 2\n",
    "    file.write(str(i) + '\\t' + str(j) + '\\n')  # 一定要记住，文本文件读进来以后还是字符串，不能直接计算\n",
    "    # write和print不一样，不能用sep和end\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f9c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件\n",
    "myfile = open('D:\\\\my study\\\\Python Study\\\\Lesson\\\\Practice2.txt','w')\n",
    "\n",
    "# 写入文件\n",
    "for i in range(1,100,2):\n",
    "    myfile.write(str(i) + '\\n')\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4665c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', '5', '7', '9', '11', '13', '15', '17', '19', '21', '23', '25', '27', '29', '31', '33', '35', '37', '39', '41', '43', '45', '47', '49', '51', '53', '55', '57', '59', '61', '63', '65', '67', '69', '71', '73', '75', '77', '79', '81', '83', '85', '87', '89', '91', '93', '95', '97', '99']\n"
     ]
    }
   ],
   "source": [
    "# # 从文本文件中读取数据\n",
    "f = open('D:\\\\my study\\\\Python Study\\\\Lesson\\\\Practice2.txt','r')\n",
    "text = f.read().strip().split('\\n')  # 感觉这个要更加权威一点  # 或者直接readlines\n",
    "print(text)\n",
    "f.close()\n",
    "\n",
    "f = open('D:\\\\my study\\\\Python Study\\\\Lesson\\\\Practice2.txt','w')\n",
    "for j in text:\n",
    "    f.write(j + '\\t' + str(float(j) ** 2) +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e763f",
   "metadata": {},
   "source": [
    "作业：请统计论文预印本网站ArXiv上，22年3月的天文学论文(astro-ph)的作者国家分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de04376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 1501 个 .tex 文件，开始处理...\n",
      "已处理 100/1501 篇论文...\n",
      "已处理 200/1501 篇论文...\n",
      "已处理 300/1501 篇论文...\n",
      "已处理 400/1501 篇论文...\n",
      "已处理 500/1501 篇论文...\n",
      "已处理 600/1501 篇论文...\n",
      "已处理 700/1501 篇论文...\n",
      "已处理 800/1501 篇论文...\n",
      "已处理 900/1501 篇论文...\n",
      "已处理 1000/1501 篇论文...\n",
      "已处理 1100/1501 篇论文...\n",
      "已处理 1200/1501 篇论文...\n",
      "已处理 1300/1501 篇论文...\n",
      "已处理 1400/1501 篇论文...\n",
      "已处理 1500/1501 篇论文...\n",
      "已处理 1501/1501 篇论文...\n",
      "\n",
      "======================================================================\n",
      "   ArXiv 天文学论文第一单位国别分布统计结果 (V10.0 最终增强版)\n",
      "   [低于 20 篇论文的国家已合并至 'Others']\n",
      "======================================================================\n",
      "总共分析了 1501 篇论文。\n",
      "总计识别国籍的论文数: 1271 (不含 'Unknown' 论文)\n",
      "统计摘要已保存到: first_affiliation_paper_stats_summary.csv\n",
      "\n",
      "--- 国家分布统计表（前5行预览） ---\n",
      "Country/Region  Paper Count Percentage (%) Rank\n",
      "           USA          645         50.75%    1\n",
      "        Others          133         10.46%    2\n",
      "         China           93          7.32%    3\n",
      "            UK           74          5.82%    4\n",
      "       Germany           63          4.96%    5\n",
      "---------------------------------\n",
      "详细的检查表已保存到: first_affiliation_country_check.csv\n",
      "\n",
      "--- 详细表格（前10行预览） ---\n",
      "          File                                                             First Unit Detail                       Source Country (Determined)\n",
      "2203.00013.tex College of Science, Northeastern University, Boston, Massachusetts 02115, USA Affiliation Macro (Fallback)                  USA\n",
      "2203.00014.tex                                                         scottgc@princeton.edu        Email (High Priority)                  USA\n",
      "2203.00016.tex                                                          lyla.jung@anu.edu.au        Email (High Priority)            Australia\n",
      "2203.00017.tex                                                                 oh@kasi.re.kr        Email (High Priority)    Republic of Korea\n",
      "2203.00018.tex                                                  s.j.molyneux@2019.ljmu.ac.uk        Email (High Priority)                   UK\n",
      "2203.00021.tex                                                       michele.cicoli@unibo.it        Email (High Priority)                Italy\n",
      "2203.00023.tex                                                jorgecortes@astro.columbia.edu        Email (High Priority)                  USA\n",
      "2203.00024.tex                                                     federica.pompa@ific.uv.es        Email (High Priority)                Spain\n",
      "2203.00025.tex                                                       antonio.sollima@inaf.it        Email (High Priority)                Italy\n",
      "2203.00026.tex                                                         lthiele@princeton.edu        Email (High Priority)                  USA\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import sys \n",
    "import json\n",
    "\n",
    "# =========================================================================\n",
    "# === 1. 配置区域 (Configuration) ===\n",
    "# =========================================================================\n",
    "\n",
    "# 存放 .tex 文件的目录。请根据您的实际路径修改！\n",
    "TEX_DIRECTORY = 'D:\\\\my study\\\\Python Study\\\\lesson\\\\tex' \n",
    "\n",
    "# 输出文件名\n",
    "OUTPUT_DETAIL_CSV = 'first_affiliation_country_check.csv' \n",
    "OUTPUT_SUMMARY_CSV = 'first_affiliation_paper_stats_summary.csv'\n",
    "\n",
    "# 统计摘要中，低于此数量的论文将被合并到 \"Others\" 类别\n",
    "OTHERS_THRESHOLD = 20 \n",
    "\n",
    "# --- V10.0 核心配置：邮箱后缀到国家映射 ---\n",
    "EMAIL_COUNTRY_MAPPING = {\n",
    "    # 北美\n",
    "    '.edu': 'USA', '.gov': 'USA', '.us': 'USA', '.org': 'USA', 'ucar.edu': 'USA', 'caltech.edu': 'USA', 'cornell.edu': 'USA', 'columbia.edu': 'USA', \n",
    "    '.ca': 'Canada', \n",
    "    # 亚洲/欧亚\n",
    "    '.cn': 'China', '.ac.cn': 'China', '.jp': 'Japan', 'nao.ac.jp': 'Japan', 'riken.jp': 'Japan',\n",
    "    '.kr': 'Republic of Korea', 'kasi.re.kr': 'Republic of Korea', '.il': 'Israel', 'tau.ac.il': 'Israel', \n",
    "    '.in': 'India', '.ir': 'Iran', '.ac.ir': 'Iran', 'guilan.ac.ir': 'Iran', '.ru': 'Russia', 'izmiran.ru': 'Russia', \n",
    "    '.tr': 'Turkey', '.tw': 'Taiwan', \n",
    "    # 拉美\n",
    "    '.mx': 'Mexico', 'unam.mx': 'Mexico', '.ec': 'Ecuador', 'yachaytech.edu.ec': 'Ecuador', \n",
    "    '.br': 'Brazil', 'cosmo-ufes.org': 'Brazil', # 解决 Júlio C. Fabris 案例\n",
    "    # 欧洲\n",
    "    '.uk': 'UK', '.ac.uk': 'UK', '.de': 'Germany', '.fr': 'France', '.it': 'Italy', 'unibo.it': 'Italy', \n",
    "    '.es': 'Spain', '.nl': 'Netherlands', '.ch': 'Switzerland', '.pl': 'Poland', '.se': 'Sweden', '.dk': 'Denmark',\n",
    "    '.gr': 'Greece', 'uoi.gr': 'Greece', '.ro': 'Romania', '.hu': 'Hungary', '.pt': 'Portugal', 'up.pt': 'Portugal', \n",
    "    # 非洲/南半球\n",
    "    '.cm': 'Cameroon', '.edu.au': 'Australia', '.ac.za': 'South Africa', '.cl': 'Chile', \n",
    "}\n",
    "\n",
    "# 扩展的国家关键词列表 (V10.0 重点增强 USA, Brazil)\n",
    "COUNTRY_KEYWORDS = {\n",
    "    # 北美 (V10.0 增强: 增加 FL, CO, MA, 91125, Cambridge)\n",
    "    r'USA': [r'USA', r'U.S.A.', r'United States', r'US', r'MA 02138', r'CT 06520', r'MD 20771', r'CA 91125', r'NJ 08544', r'WA, 98195', r'Baltimore, MD 21218', r'Stanford, CA 94305', r'College Park, MD 20742', r'New Haven, CT 06520', r'Pasadena CA-91101', r'Boulder, Colorado 80301', r'Ann Arbor, MI 48109', r'Charlottesville, VA 22901', r'Urbana, IL 61801', r'Toledo, OH 43606', r'New York, NY', r'New York', r'NY', r'MA', r'CA', r'TX', r'IL', r'CO 80309', r'Cambridge, MA', r'91125', r'FL., 32114', r'FL', r'Daytona Beach'], \n",
    "    r'Canada': [r'Canada', r'BC, V8P 1A1', r'V8P 1A1', r'Victoria, BC', r'V8P 1A1', r'Toronto'],\n",
    "    \n",
    "    # 亚洲/欧亚\n",
    "    r'China': [r'China', r'P. R. China', r'PRC', r'Chinese Academy of Sciences', r'Beijing 100871', r'Peking University', r'Hangzhou 310058', r'Hangzhou 310018'], \n",
    "    r'India': [r'India', r'West Bengal 721302', r'721302, India', r'Kharagpur'],\n",
    "    r'Republic of Korea': [r'Republic of Korea', r'Seoul 03722', r'South Korea', r'03722', r'Korea Ast', r'Daejeon 34055'], \n",
    "    r'Israel': [r'Israel', r'Tel Aviv 69978', r'Be\\'er-Sheva 8410501', r'8410501', r'Ben-Gurion University'], \n",
    "    r'Japan': [r'Japan', r'Kyoto', r'Tokyo', r'RIKEN', r'Sendai 980-8578', r'Kyoto 606-8502', r'Sakyo-ku, Kyoto', r'Mitaka', r'181-8588'], \n",
    "    r'Iran': [r'Iran', r'Rasht'],\n",
    "    r'Russia': [r'Russia', r'Russian Federation'],\n",
    "    r'Turkey': [r'Turkey', r'ankara.edu.tr'],\n",
    "    r'Taiwan': [r'Taiwan', r'Taipei 10617', r'Taichung, Taiwan'], \n",
    "    \n",
    "    # 拉美\n",
    "    r'Mexico': [r'Mexico', r'M\\'exico', r'CDMX 04510', r'Guadalajara, Jalisco', r'44430', r'45100', r'36000'], \n",
    "    r'Ecuador': [r'Ecuador', r'Urcuqu\\'i', r'100119', r'Yachay Tech University'], \n",
    "    r'Chile': [r'Chile', r'Santiago', r'Antofagasta', r'Casilla 36-D', r'N\\'ucleo de Astronom\\'{\\i}a'], \n",
    "    r'Brazil': [r'Brazil', r'S\\~ao Paulo', r'Rio Grande do Sul', r'Vitória', r'UFES', r'Espírito'], \n",
    "    \n",
    "    # 欧洲\n",
    "    r'UK': [r'UK', r'United Kingdom', r'GB', r'SK11 9FT'], \n",
    "    r'Italy': [r'Italy', r'Bologna, Italy', r'40126 Bologna', r'40127 Bologna', r'40129 Bologna', r'35122 Padova', r'Monte Porzio Catone', r'70126 Bari'],\n",
    "    r'Germany': [r'Germany', r'Garching, Germany'], \n",
    "    r'Switzerland': [r'Switzerland', r'Versoix, Switzerland', r'8093 Zurich'], \n",
    "    r'France': [r'France', r'Toulouse, France', r'31400 Toulouse', r'F-76058 Le Havre', r'F-91405 Orsay'], \n",
    "    r'Netherlands': [r'Netherlands', r'Leiden, Netherlands'],\n",
    "    r'Spain': [r'Spain', r'Madrid, Spain', r'28850 Torrej\\'{o}n de Ardoz'], \n",
    "    r'Greece': [r'Greece', r'GR-45110 Ioannina', r'45110 Ioannina', r'Zografos, Greece'],\n",
    "    r'Romania': [r'Romania', r'300223 Timi\\c{s}oara', r'300223, Timisoara'], \n",
    "    r'Hungary': [r'Hungary', r'H-4001 Debrecen', r'4001 Debrecen'],\n",
    "    r'Portugal': [r'Portugal', r'4200-465 Porto', r'1049-001, Lisbon'],\n",
    "\n",
    "    # 非洲/南半球\n",
    "    r'Cameroon': [r'Cameroon', r'Douala, Cameroon', r'Maroua, Cameroon'],\n",
    "    r'Australia': [r'Australia', r'ACT 2611', r'WA 6845'], \n",
    "    r'South Africa': [r'South Africa'],\n",
    "}\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# === 2. 辅助函数 (Helper Functions) ===\n",
    "# =========================================================================\n",
    "\n",
    "def extract_country(text, is_email=False):\n",
    "    \"\"\"\n",
    "    V10.0 修正：增强 LaTeX 符号和非 ASCII 字符的清理，避免误删关键词。\n",
    "    \"\"\"\n",
    "    # 1. 邮箱后缀匹配 (高优先级)\n",
    "    if is_email:\n",
    "        match = re.search(r'@(.*)', text)\n",
    "        if match:\n",
    "            domain = match.group(1).lower()\n",
    "            if domain in EMAIL_COUNTRY_MAPPING:\n",
    "                return EMAIL_COUNTRY_MAPPING[domain]\n",
    "            for suffix, country in EMAIL_COUNTRY_MAPPING.items():\n",
    "                if domain.endswith(suffix) and suffix.startswith('.'):\n",
    "                    return country\n",
    "    \n",
    "    # 2. 地址关键词匹配 (低优先级)\n",
    "    address_clean = (text.lower()\n",
    "                    .replace(r'\\\\', ' ')\n",
    "                    .replace(r'{', ' ')\n",
    "                    .replace(r'}', ' ')\n",
    "                    .replace(r'$', ' ')\n",
    "                    .replace(r'\\n', ' ')\n",
    "                    .replace(r'\\r', ' ')\n",
    "                    \n",
    "                    # V10.0 修正: 集中处理常见的 LaTeX 特殊字符转义\n",
    "                    .replace(r'\\\"u', 'u').replace(r'\\'i', 'i').replace(r'\\i', 'i')   \n",
    "                    .replace(r'\\'e', 'e').replace(r'\\\"a', 'a').replace(r'\\\"o', 'o')\n",
    "                    .replace(r'\\'a', 'a').replace(r'\\'o', 'o').replace(r'\\~{n}', 'n') \n",
    "                    .replace(r'\\'e', 'e').replace(r'\\'{\\i}', 'i').replace(r'\\`{a}', 'a') \n",
    "                    .replace(r'\\c{s}', 's') \n",
    "                    .replace(r'\\&', ' and ') # \\ & 替换为 and\n",
    "                    .replace(r'~', ' ')\n",
    "                    .strip())\n",
    "    \n",
    "    # V10.0 修正: 清理 \\ding{...}、\\thanks 及其它宏\n",
    "    address_clean = re.sub(r'\\\\(?:ding|orcidlink|email|href|inst|def|corrEmail|and|affil|thanks)\\s*\\{.*?\\}|\\s*\\\\email\\s*\\{.*?\\}|\\s*\\\\author\\s*\\{.*?\\}', ' ', address_clean) \n",
    "    \n",
    "    # V10.0 新增: 清理非 ASCII 字符 (如 Júlio 中的 ú)\n",
    "    address_clean = re.sub(r'[^\\x00-\\x7F]+', ' ', address_clean) \n",
    "\n",
    "    # 最终去除所有剩余的反斜杠符号，但保留空格\n",
    "    address_clean = address_clean.replace('\\\\', ' ')\n",
    "    \n",
    "    for country, keywords in COUNTRY_KEYWORDS.items():\n",
    "        for keyword in keywords:\n",
    "            # 关键词清理与地址清理一致\n",
    "            key = keyword.lower().replace(r'\\\\', ' ').replace(r'{', ' ').replace(r'}', ' ').replace(r'$', ' ')\n",
    "            if key in address_clean.split() or key in address_clean:\n",
    "                 return country\n",
    "                 \n",
    "    return \"Unknown\"\n",
    "\n",
    "# 获取文件内容 (保持不变)\n",
    "def get_tex_content(directory):\n",
    "    all_content = []\n",
    "    file_paths = glob.glob(os.path.join(directory, r\"*.tex\"))\n",
    "    if not file_paths:\n",
    "        print(f\"警告: 在目录 '{directory}' 中未找到任何 .tex 文件。\")\n",
    "        return []\n",
    "    print(f\"找到 {len(file_paths)} 个 .tex 文件，开始处理...\")\n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                all_content.append((file_name, content))\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_name} 时出错: {e}\")\n",
    "    return all_content\n",
    "\n",
    "# =========================================================================\n",
    "# === 3. 核心提取函数 (V10.0 - 新增 \\thanks 匹配) ===\n",
    "# =========================================================================\n",
    "\n",
    "def extract_first_country_info(content):\n",
    "    \n",
    "    # 邮箱正则表达式匹配 (最高优先级)\n",
    "    email_pattern = re.compile(\n",
    "        r'\\\\email\\s*\\{(.*?)\\}'                      \n",
    "        r'|\\\\emailAdd\\s*\\{(.*?)\\}'                   \n",
    "        r'|\\\\author\\s*\\[.*?email\\s*=\\s*\\{(.*?)\\}.*?\\]' \n",
    "        r'|\\\\def\\\\corrEmail\\s*\\{(.*?)\\}'             \n",
    "        r'|\\\\institute\\s*\\{.*?\\\\email\\s*\\{(.*?)\\}.*?\\}' \n",
    "        r'|\\\\thanks\\s*\\{.*?([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\}' \n",
    "        r'|\\\\correspondingauthor\\s*\\{.*?\\}\\s*\\\\email\\s*\\{(.*?)\\}' \n",
    "        r'|E-mail:\\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    email_match = email_pattern.search(content)\n",
    "    \n",
    "    if email_match:\n",
    "        email = next((g.strip() for g in email_match.groups() if g is not None and g.strip()), None)\n",
    "        \n",
    "        if email:\n",
    "            email = re.sub(r'\\(.*?\\)|\\\\href\\{mailto:|\\}', '', email).strip()\n",
    "            country = extract_country(email, is_email=True)\n",
    "            if country != \"Unknown\":\n",
    "                return {\n",
    "                    'Source': 'Email (High Priority)',\n",
    "                    'Detail': email,\n",
    "                    'Country': country\n",
    "                }\n",
    "\n",
    "    # 其次是 \\thanks 宏匹配 (V10.0 新增)\n",
    "    thanks_pattern = re.compile(\n",
    "        r'\\\\author.*?\\\\thanks\\s*\\{(.*?)\\}', \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    thanks_match = thanks_pattern.search(content)\n",
    "\n",
    "    if thanks_match:\n",
    "        thanks_text = next((g.strip() for g in thanks_match.groups() if g is not None and g.strip()), None)\n",
    "        \n",
    "        if thanks_text:\n",
    "            # 移除所有 \\and 及其后的内容 (通常 \\thanks 不会有 \\and)\n",
    "            thanks_text = re.split(r'\\\\and', thanks_text, 1)[0]\n",
    "            \n",
    "            # 使用关键词判断国别\n",
    "            country = extract_country(thanks_text, is_email=False)\n",
    "            \n",
    "            if country != \"Unknown\":\n",
    "                return {\n",
    "                    'Source': 'Thanks Macro (Second Priority)',\n",
    "                    'Detail': thanks_text,\n",
    "                    'Country': country\n",
    "                }\n",
    "\n",
    "    # 最后回退到 \\affiliation 等单位地址宏匹配\n",
    "    affil_pattern = re.compile(\n",
    "        r'\\\\(?:affiliation|affil|address|institute|aff)\\s*(?:\\[.*?\\])?\\s*\\{(.*?)\\}' \n",
    "        r'|\\\\def\\\\Address\\s*\\{(.*?)\\}'                  \n",
    "        r'|\\\\altaffiltext\\s*\\{.*?\\}\\s*\\{(.*?)\\}'         \n",
    "        r'|\\$\\^\\{?(\\d+)\\}?\\$.*?\\n\\s*(\\^\\{?(\\d+)\\}?\\$.*?)\\n\\n', \n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    affil_match = affil_pattern.search(content)\n",
    "\n",
    "    if affil_match:\n",
    "        affil_text = next((g.strip() for g in affil_match.groups() if g is not None and g.strip()), None)\n",
    "\n",
    "        if affil_text:\n",
    "            # 提取第一个单位，移除所有 \\and 及其后的内容\n",
    "            affil_text = re.split(r'\\\\and', affil_text, 1)[0]\n",
    "            \n",
    "            # 清理复杂块的编号 (如 $^1$ )\n",
    "            affil_text = re.sub(r'\\$\\^\\{?(\\d+)\\}?\\$\\s*', '', affil_text, flags=re.DOTALL)\n",
    "            \n",
    "            # 针对 \\normalsize 块的特殊处理\n",
    "            if re.match(r'^\\\\normalsize\\{', affil_text):\n",
    "                 affil_text = re.sub(r'\\\\normalsize\\{(.*?)\\}', r'\\1', affil_text, flags=re.DOTALL)\n",
    "            \n",
    "            # 使用关键词判断国别\n",
    "            country = extract_country(affil_text, is_email=False)\n",
    "            \n",
    "            return {\n",
    "                'Source': 'Affiliation Macro (Fallback)',\n",
    "                'Detail': affil_text,\n",
    "                'Country': country\n",
    "            }\n",
    "            \n",
    "    return {\n",
    "        'Source': 'N/A',\n",
    "        'Detail': 'No valid email, thanks, or affiliation macro found.',\n",
    "        'Country': 'Unknown'\n",
    "    }\n",
    "\n",
    "# =========================================================================\n",
    "# === 4. 统计与主程序 (Analysis and Main Execution) ===\n",
    "# =========================================================================\n",
    "\n",
    "def analyze_tex_files_for_table(tex_contents):\n",
    "    # ... (保持 V9.8 的统计逻辑不变)\n",
    "    country_counts = defaultdict(int)\n",
    "    details_list = [] \n",
    "    total_papers = len(tex_contents)\n",
    "    paper_country_map = {} \n",
    "\n",
    "    for i, (file_name, content) in enumerate(tex_contents):\n",
    "        \n",
    "        info = extract_first_country_info(content)\n",
    "        fa_country_name = info['Country']\n",
    "        \n",
    "        if file_name not in paper_country_map:\n",
    "            country_counts[fa_country_name] += 1\n",
    "            paper_country_map[file_name] = fa_country_name\n",
    "            \n",
    "            details_list.append({\n",
    "                'File': file_name,\n",
    "                'First Unit Detail': info['Detail'],\n",
    "                'Source': info['Source'],\n",
    "                'Country (Determined)': fa_country_name\n",
    "            })\n",
    "            \n",
    "        if (i + 1) % 100 == 0 or (i + 1) == total_papers:\n",
    "            print(f\"已处理 {i + 1}/{total_papers} 篇论文...\")\n",
    "            \n",
    "    total_identified_papers = sum(count for country, count in country_counts.items() if country != \"Unknown\")\n",
    "            \n",
    "    sorted_counts_list = sorted(country_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return sorted_counts_list, total_identified_papers, details_list, total_papers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        pd.DataFrame() \n",
    "    except (NameError, AttributeError):\n",
    "        print(\"错误：无法初始化 pandas DataFrame。请检查 pandas 库是否正确安装和导入。\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    all_tex_contents_with_name = get_tex_content(TEX_DIRECTORY) \n",
    "    \n",
    "    if all_tex_contents_with_name:\n",
    "        results_counts_list, total_identified_papers, details_list, total_papers = analyze_tex_files_for_table(all_tex_contents_with_name)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"   ArXiv 天文学论文第一单位国别分布统计结果 (V10.0 最终增强版)\")\n",
    "        print(f\"   [低于 {OTHERS_THRESHOLD} 篇论文的国家已合并至 'Others']\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if total_identified_papers > 0:\n",
    "            summary_data = []\n",
    "            others_count = 0\n",
    "            total_base = total_identified_papers\n",
    "            unknown_count = next((count for c, count in results_counts_list if c == \"Unknown\"), 0)\n",
    "            \n",
    "            for country, count in results_counts_list:\n",
    "                if country == \"Unknown\": continue \n",
    "                if count >= OTHERS_THRESHOLD:\n",
    "                    percentage = (count / total_base) * 100\n",
    "                    summary_data.append({'Country/Region': country, 'Paper Count': count, 'Percentage (%)': percentage})\n",
    "                else:\n",
    "                    others_count += count\n",
    "            \n",
    "            if others_count > 0:\n",
    "                others_percentage = (others_count / total_base) * 100\n",
    "                summary_data.append({'Country/Region': 'Others', 'Paper Count': others_count, 'Percentage (%)': others_percentage})\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df = summary_df.sort_values(by='Paper Count', ascending=False).reset_index(drop=True)\n",
    "            summary_df['Rank'] = summary_df.index + 1\n",
    "            summary_df['Percentage (%)'] = summary_df['Percentage (%)'].map(lambda x: f'{x:.2f}%')\n",
    "            \n",
    "            unknown_row = pd.DataFrame([{'Rank': 'N/A', 'Country/Region': 'Unknown', 'Paper Count': unknown_count, 'Percentage (%)': 'N/A'}])\n",
    "            summary_df = pd.concat([summary_df, unknown_row], ignore_index=True)\n",
    "            \n",
    "            summary_df.to_csv(OUTPUT_SUMMARY_CSV, index=False, encoding='utf-8')\n",
    "            print(f\"总共分析了 {total_papers} 篇论文。\")\n",
    "            print(f\"总计识别国籍的论文数: {total_identified_papers} (不含 'Unknown' 论文)\")\n",
    "            print(f\"统计摘要已保存到: {OUTPUT_SUMMARY_CSV}\")\n",
    "\n",
    "            print(\"\\n--- 国家分布统计表（前5行预览） ---\")\n",
    "            print(summary_df.head(5).to_string(index=False))\n",
    "            print(\"---------------------------------\")\n",
    "            \n",
    "        if details_list:\n",
    "            df_detail = pd.DataFrame(details_list)\n",
    "            df_detail = df_detail.sort_values(by=['File']).reset_index(drop=True)\n",
    "            df_detail['First Unit Detail'] = df_detail['First Unit Detail'].astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "            df_detail.to_csv(OUTPUT_DETAIL_CSV, index=False, encoding='utf-8')\n",
    "            \n",
    "            print(f\"详细的检查表已保存到: {OUTPUT_DETAIL_CSV}\")\n",
    "            print(\"\\n--- 详细表格（前10行预览） ---\")\n",
    "            print(df_detail.head(10).to_string(index=False))\n",
    "            print(\"---------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
